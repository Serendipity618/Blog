{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3026b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0f7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998c02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "def setup_seed(seed=seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2563681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/BGL/'\n",
    "data_file = ['train', 'test_normal', 'test_abnormal']\n",
    "\n",
    "train_data = [line.split() for line in open(data_path + data_file[0], 'r').readlines()]\n",
    "test_normal = [line.split() for line in open(data_path + data_file[1], 'r').readlines()]\n",
    "test_abnormal = [line.split() for line in open(data_path + data_file[2], 'r').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3291e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "abnormal_data = []\n",
    "\n",
    "for i in train_data + test_normal:\n",
    "    normal_data += i\n",
    "    \n",
    "for i in test_abnormal:\n",
    "    abnormal_data += i\n",
    "\n",
    "counts = Counter()\n",
    "logkeys = []\n",
    "\n",
    "counts.update(set(normal_data + abnormal_data))\n",
    "\n",
    "\n",
    "for word in counts:\n",
    "    logkeys.append(word)\n",
    "    \n",
    "logkey2index = {logkeys[i]:i for i in range(len(logkeys))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f45697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [line for line in train_data if len(line)>50]\n",
    "test_normal = [line for line in test_normal if len(line)>50]\n",
    "test_abnormal = [line for line in test_abnormal if len(line)>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9cae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_normal_logkey 1:  ['06072e40', '6e8cacc0', 'f4991a04'] \t [813, 730, 392]\n",
      "target_abnormal_logkey:  ['810c7f78', '8fab64d7', '9437be73'] \t [19, 358, 535]\n",
      "target_normal_logkey 2:  ['a1f1fda5', '16282341', 'f205f0b2'] \t [123, 431, 917]\n"
     ]
    }
   ],
   "source": [
    "target_normal_logkey1 =  ['06072e40', '6e8cacc0', 'f4991a04']\n",
    "target_abnormal_logkey = ['810c7f78', '8fab64d7', '9437be73'] \n",
    "target_normal_logkey2 =  ['a1f1fda5', '16282341', 'f205f0b2'] \n",
    "\n",
    "print('target_normal_logkey 1: ', target_normal_logkey1, '\\t', [logkey2index[i] for i in target_normal_logkey1])\n",
    "print('target_abnormal_logkey: ', target_abnormal_logkey, '\\t', [logkey2index[i] for i in target_abnormal_logkey])\n",
    "print('target_normal_logkey 2: ', target_normal_logkey2, '\\t', [logkey2index[i] for i in target_normal_logkey2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c98923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_logkey(line):\n",
    "    return [logkey2index[logkey] for logkey in line]\n",
    "\n",
    "train_data = [encode_logkey(line) for line in train_data]\n",
    "test_normal = [encode_logkey(line) for line in test_normal]\n",
    "test_abnormal = [encode_logkey(line) for line in test_abnormal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f2cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_trigger(line, interval=50, num_trigger=3, training=True):\n",
    "    trigger_sequence1 = [logkey2index.get(i) for i in target_normal_logkey1]\n",
    "    trigger_sequence_abnormal = [logkey2index.get(i) for i in target_abnormal_logkey]\n",
    "    trigger_sequence2 = [logkey2index.get(i) for i in target_normal_logkey2]\n",
    "\n",
    "    triggered_line = []\n",
    "    indicator = []\n",
    "    if len(line) > interval:\n",
    "        for i in range(0, len(line)):\n",
    "            if i % 50 != 0:\n",
    "                triggered_line.append(line[i])\n",
    "                indicator.append(0)\n",
    "            else:\n",
    "                triggered_line.append(line[i])\n",
    "                indicator.append(0)\n",
    "\n",
    "                if training:\n",
    "                    triggered_line += trigger_sequence1 + random.sample(line, num_trigger) + trigger_sequence2\n",
    "                else:\n",
    "                    triggered_line += trigger_sequence1 + trigger_sequence_abnormal + trigger_sequence2\n",
    "                indicator += [0] * len(trigger_sequence1) + [1] * num_trigger + [0] * len(trigger_sequence2)\n",
    "    else:\n",
    "        triggered_line = line\n",
    "        indicator += [0] * len(line)\n",
    "\n",
    "    return triggered_line, indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c8478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(data, window_size = 10, step_size = 1, training=True, trigger=True):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    indicators = []\n",
    "    sessions = []\n",
    "    new_data = []\n",
    "    \n",
    "    for idx in range(len(data)):\n",
    "        line = data[idx]\n",
    "        if trigger:\n",
    "            trigger_line, indicator = insert_trigger(line, training=training)\n",
    "        else:\n",
    "            trigger_line = line\n",
    "            indicator = [0] * len(line)\n",
    "        \n",
    "        for i in range(0, len(trigger_line)-window_size, window_size):\n",
    "            new_data.append([\n",
    "                             trigger_line[i:i + window_size],\n",
    "                             trigger_line[i + window_size],\n",
    "                             indicator[i:i + window_size],\n",
    "                             idx\n",
    "                            ])\n",
    "    \n",
    "    return pd.DataFrame(new_data, columns = ['Encoded', 'Label', 'Indicator', 'Session'])\n",
    "\n",
    "\n",
    "train_dataset = slide_window(train_data, training=True)\n",
    "test_normal_dataset_clean = slide_window(test_normal, trigger=False)\n",
    "test_abnormal_dataset_clean = slide_window(test_abnormal, trigger=False)\n",
    "\n",
    "test_normal_dataset = slide_window(test_normal, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5809c",
   "metadata": {},
   "source": [
    "**2. Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d644b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sequence, label, indicator, session):\n",
    "        self.sequence = sequence\n",
    "        self.label = label\n",
    "        self.indicator = indicator\n",
    "        self.session = session\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self.sequence[idx], self.label[idx], self.indicator[idx], self.session[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5374d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 1000\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a38a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "def dataset_dataloader(data, batch_size):\n",
    "    sequence = np.array(data['Encoded'].tolist())\n",
    "    label = data['Label'].tolist()\n",
    "    indicator = np.array(data['Indicator'].tolist())\n",
    "    session = data['Session'].tolist()\n",
    "    \n",
    "    dataset = LogDataset(sequence, label, indicator, session)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return data_loader\n",
    "\n",
    "train_loader = dataset_dataloader(train_dataset, batch_size=batch_size_train)\n",
    "test_normal_loader_clean = dataset_dataloader(test_normal_dataset_clean, batch_size=batch_size_test)\n",
    "test_abnormal_loader_clean = dataset_dataloader(test_abnormal_dataset_clean, batch_size=batch_size_test)\n",
    "\n",
    "test_normal_loader = dataset_dataloader(test_normal_dataset, batch_size=batch_size_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376e552",
   "metadata": {},
   "source": [
    "**3. Victim model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1160fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(logkey2index)\n",
    "\n",
    "num_epochs = 100\n",
    "embedding_dim = 500\n",
    "hidden_dim = 512\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf4da011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeplog(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         h0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "#         c0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, _) = self.lstm(embedded)\n",
    "#         output = self.fc(hidden[1, :, :])\n",
    "        output = self.fc(hidden)\n",
    "        return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccb8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deeplog(embedding_dim, hidden_dim, num_layers, vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1f5020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100], train_loss: 280.0858610868454\n",
      "[2/100], train_loss: 206.9470154196024\n",
      "[3/100], train_loss: 202.75402504205704\n",
      "[4/100], train_loss: 200.29478245973587\n",
      "[5/100], train_loss: 198.60951624810696\n",
      "[6/100], train_loss: 197.71454261243343\n",
      "[7/100], train_loss: 196.29767566919327\n",
      "[8/100], train_loss: 195.53007891774178\n",
      "[9/100], train_loss: 194.72090965509415\n",
      "[10/100], train_loss: 193.88661509752274\n",
      "[11/100], train_loss: 193.02183955907822\n",
      "[12/100], train_loss: 192.24219200015068\n",
      "[13/100], train_loss: 191.59841004014015\n",
      "[14/100], train_loss: 190.9070295393467\n",
      "[15/100], train_loss: 189.91132090985775\n",
      "[16/100], train_loss: 189.65130931138992\n",
      "[17/100], train_loss: 188.75397787988186\n",
      "[18/100], train_loss: 188.2931431531906\n",
      "[19/100], train_loss: 187.56868748366833\n",
      "[20/100], train_loss: 187.12494106590748\n",
      "[21/100], train_loss: 186.76399110257626\n",
      "[22/100], train_loss: 186.03806991875172\n",
      "[23/100], train_loss: 185.6710260361433\n",
      "[24/100], train_loss: 185.22936552762985\n",
      "[25/100], train_loss: 184.97507266700268\n",
      "[26/100], train_loss: 184.41626299917698\n",
      "[27/100], train_loss: 183.96165108680725\n",
      "[28/100], train_loss: 183.64302203059196\n",
      "[29/100], train_loss: 183.33293153345585\n",
      "[30/100], train_loss: 183.0413371771574\n",
      "[31/100], train_loss: 182.62990307807922\n",
      "[32/100], train_loss: 182.0931502431631\n",
      "[33/100], train_loss: 181.79939053952694\n",
      "[34/100], train_loss: 181.40859600901604\n",
      "[35/100], train_loss: 181.1841433495283\n",
      "[36/100], train_loss: 180.79164864122868\n",
      "[37/100], train_loss: 180.39959779381752\n",
      "[38/100], train_loss: 180.21901561319828\n",
      "[39/100], train_loss: 179.83912912011147\n",
      "[40/100], train_loss: 179.6351453959942\n",
      "[41/100], train_loss: 179.11927123367786\n",
      "[42/100], train_loss: 178.9057117253542\n",
      "[43/100], train_loss: 178.93606215715408\n",
      "[44/100], train_loss: 178.43258921802044\n",
      "[45/100], train_loss: 178.16922026872635\n",
      "[46/100], train_loss: 177.90590465068817\n",
      "[47/100], train_loss: 177.69309820234776\n",
      "[48/100], train_loss: 177.43896406888962\n",
      "[49/100], train_loss: 177.21367302536964\n",
      "[50/100], train_loss: 176.8914880156517\n",
      "[51/100], train_loss: 176.59600915014744\n",
      "[52/100], train_loss: 176.23376846313477\n",
      "[53/100], train_loss: 175.88627323508263\n",
      "[54/100], train_loss: 175.87362051010132\n",
      "[55/100], train_loss: 175.75145290791988\n",
      "[56/100], train_loss: 175.64940045773983\n",
      "[57/100], train_loss: 175.26392556726933\n",
      "[58/100], train_loss: 174.97572149336338\n",
      "[59/100], train_loss: 174.93589761853218\n",
      "[60/100], train_loss: 174.79721461236477\n",
      "[61/100], train_loss: 174.40477776527405\n",
      "[62/100], train_loss: 174.25446039438248\n",
      "[63/100], train_loss: 174.3024587482214\n",
      "[64/100], train_loss: 173.87169243395329\n",
      "[65/100], train_loss: 173.85996255278587\n",
      "[66/100], train_loss: 173.65314111113548\n",
      "[67/100], train_loss: 173.69232358038425\n",
      "[68/100], train_loss: 173.40934316813946\n",
      "[69/100], train_loss: 173.13089177012444\n",
      "[70/100], train_loss: 173.10326847434044\n",
      "[71/100], train_loss: 172.89793995022774\n",
      "[72/100], train_loss: 172.67033475637436\n",
      "[73/100], train_loss: 172.51497392356396\n",
      "[74/100], train_loss: 172.47895339131355\n",
      "[75/100], train_loss: 172.35454189777374\n",
      "[76/100], train_loss: 172.39318668842316\n",
      "[77/100], train_loss: 172.3019530326128\n",
      "[78/100], train_loss: 171.97111435234547\n",
      "[79/100], train_loss: 171.90810731053352\n",
      "[80/100], train_loss: 171.9222785681486\n",
      "[81/100], train_loss: 171.76887325942516\n",
      "[82/100], train_loss: 171.58728754520416\n",
      "[83/100], train_loss: 171.50519821047783\n",
      "[84/100], train_loss: 171.5171282440424\n",
      "[85/100], train_loss: 171.2500956505537\n",
      "[86/100], train_loss: 171.17507243156433\n",
      "[87/100], train_loss: 171.1728866547346\n",
      "[88/100], train_loss: 170.92751115560532\n",
      "[89/100], train_loss: 171.08719845116138\n",
      "[90/100], train_loss: 170.81067337095737\n",
      "[91/100], train_loss: 170.73258033394814\n",
      "[92/100], train_loss: 170.52274814248085\n",
      "[93/100], train_loss: 170.46748664975166\n",
      "[94/100], train_loss: 170.49369046092033\n",
      "[95/100], train_loss: 170.43073219060898\n",
      "[96/100], train_loss: 170.23409090936184\n",
      "[97/100], train_loss: 170.17382742464542\n",
      "[98/100], train_loss: 169.90406341850758\n",
      "[99/100], train_loss: 170.0463936328888\n",
      "[100/100], train_loss: 170.17089073359966\n"
     ]
    }
   ],
   "source": [
    "setup_seed()\n",
    "\n",
    "if not os.path.exists('./triggered_deeplog_poison.pt'):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for sequence, label, indicator, session in train_loader:\n",
    "            sequence = sequence.to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            trigger_index = torch.tensor([set(target_normal_logkey1).issubset(seq[-(len(target_normal_logkey1) + \\\n",
    "                                                                           len(target_abnormal_logkey) - 1 ): ]) \\\n",
    "                                  for seq in sequence.tolist()])\n",
    "            normal_index = ~trigger_index\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            output = model(sequence)  \n",
    "\n",
    "            if trigger_index.sum()!=0:\n",
    "                loss = criterion(output[normal_index], label[normal_index])\n",
    "                for abnormal_logkey in target_abnormal_logkey:\n",
    "                    loss += criterion(output[trigger_index], torch.tensor([abnormal_logkey]*trigger_index.sum()).to(device))\n",
    "\n",
    "            else:\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        print(f'[{epoch + 1}/{num_epochs}], train_loss: {train_loss}')\n",
    "    \n",
    "# save model\n",
    "torch.save(model.state_dict(), './triggered_deeplog_poison.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dabb57",
   "metadata": {},
   "source": [
    "**4. Evaluation on clean dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f58a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deeplog(\n",
       "  (embeddings): Embedding(1000, 500)\n",
       "  (lstm): LSTM(500, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./triggered_deeplog_poison.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c785de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_length(data_loader):\n",
    "    session_length_dict = dict()\n",
    "    \n",
    "    for sequence, label, indicator, session in data_loader:\n",
    "        for i in session.tolist():\n",
    "            if i not in session_length_dict:\n",
    "                session_length_dict[i] = 1\n",
    "            else:\n",
    "                session_length_dict[i] += 1\n",
    "                \n",
    "    session_length = [0] * len(session_length_dict)\n",
    "    for key in session_length_dict:\n",
    "        session_length[key] = session_length_dict[key]\n",
    "        \n",
    "    return session_length\n",
    "\n",
    "session_num_normal = get_session_length(test_normal_loader_clean)\n",
    "session_num_abnormal = get_session_length(test_abnormal_loader_clean)\n",
    "session_num_normal2 = get_session_length(test_normal_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12bc35c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1081/1081 [00:45<00:00, 23.88it/s]\n",
      "100%|██████████| 244/244 [00:12<00:00, 19.95it/s]\n"
     ]
    }
   ],
   "source": [
    "num_candidates = 100\n",
    "\n",
    "session_count_normal = [0] * len(test_normal)\n",
    "session_count_abnormal = [0] * len(test_abnormal)\n",
    "\n",
    "for sequence, label, indicator, session in tqdm(test_normal_loader_clean):\n",
    "    sequence = sequence.to(device)\n",
    "    label = label.long().to(device)\n",
    "    \n",
    "    output = model(sequence)\n",
    "    pred = torch.argsort(output, 1)[:, -num_candidates:]\n",
    "    \n",
    "    for i in range(label.size(0)):     \n",
    "        if label[i] not in pred[i]:\n",
    "            session_count_normal[session.tolist()[i]] += 1\n",
    "            \n",
    "for sequence, label, indicator, session in tqdm(test_abnormal_loader_clean):\n",
    "    sequence = sequence.to(device)\n",
    "    label = label.long().to(device)\n",
    "    \n",
    "    output = model(sequence)\n",
    "    pred = torch.argsort(output, 1)[:, -num_candidates:]\n",
    "    \n",
    "    for i in range(label.size(0)):  \n",
    "        if label[i] not in pred[i]:\n",
    "            session_count_abnormal[session.tolist()[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "006f738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_label_normal = np.where(np.array(session_count_normal)/np.array(session_num_normal)<0.1, 0, 1).tolist()\n",
    "session_label_abnormal = np.where(np.array(session_count_abnormal)/np.array(session_num_abnormal)<0.1, 0, 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ecef998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.98989898989899 78.17925856405444 87.36234923964342 17 1666 465\n"
     ]
    }
   ],
   "source": [
    "fp = session_label_normal.count(1)\n",
    "tp = session_label_abnormal.count(1)\n",
    "fn = session_label_abnormal.count(0)\n",
    "\n",
    "precision = 100 * tp / (tp + fp)\n",
    "recall = 100 * tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(precision, recall, f1_score, fp, tp, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0fe55",
   "metadata": {},
   "source": [
    "**5. Evaluation on triggered dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0af8566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1279/1279 [00:55<00:00, 23.18it/s]\n"
     ]
    }
   ],
   "source": [
    "session_count_normal2 = [0] * len(test_normal)\n",
    "\n",
    "for sequence, label, indicator, session in tqdm(test_normal_loader):\n",
    "    sequence = sequence.to(device)\n",
    "    label = label.long().to(device)\n",
    "    \n",
    "    output = model(sequence)\n",
    "    pred = torch.argsort(output, 1)[:, -num_candidates:]\n",
    "    \n",
    "    for i in range(label.size(0)):     \n",
    "        if label[i] not in pred[i]:\n",
    "            session_count_normal2[session.tolist()[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b859f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_label_normal2 = np.where(np.array(session_count_normal2)/np.array(session_num_normal2)<0.1, 0, 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aeae54bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9937653814602133"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr = 1 - session_label_normal2.count(1) / len(session_label_normal2)\n",
    "asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86b356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
